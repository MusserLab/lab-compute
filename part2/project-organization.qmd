# Project Organization

In [Your First R Project](../part1/first-project.qmd), you created a project with `data/`, `scripts/`, and `outs/` folders—the same structure used in every lab project. This chapter explains that structure formally and covers the conventions that keep projects organized as they grow.

A well-organized project makes it easy to find files, understand dependencies, and reproduce analyses. This chapter describes the lab's conventions for structuring analysis projects.

## The Core Principle

Every analysis project has two types of files:

1. **External inputs** — data that comes from outside the project (sequencing results, collaborator files, public datasets)
2. **Generated outputs** — everything produced by your scripts (processed data, figures, tables, reports)

The key to staying organized is keeping these separate and tracking which scripts produce which outputs.

## Project Structure

A typical lab project looks like this:

```
my-project/
├── data/              # External inputs only — scripts never write here
├── scripts/           # Quarto analysis scripts (.qmd)
│   └── exploratory/   # One-off analyses (no numbering required)
├── outs/              # All script-generated outputs
├── R/                 # Shared R helper functions
├── python/            # Shared Python helper functions
├── environment.yml    # Conda environment
├── renv.lock          # R package versions
└── README.md          # Project overview
```

### `data/` — External Inputs

This folder holds data that comes from **outside** the project:

- Raw sequencing data from the core facility
- Files from collaborators
- Downloaded public datasets
- Metadata spreadsheets

```
data/
├── counts_matrix.csv          # From sequencing core
├── sample_metadata.xlsx       # From collaborator
└── reference_genome.fasta     # Downloaded from NCBI
```

**Rules for `data/`:**

- Never modify these files
- Never put script-generated files here
- Document where each file came from (in the README or a `data/README.md`)

### `scripts/` — Analysis Scripts

All `.qmd` analysis scripts live here, numbered to indicate their logical order:

```
scripts/
├── 01_import_qc.qmd
├── 02_normalize.qmd
├── 03_differential.qmd
├── 04_volcano_plots.qmd
├── 05_heatmaps.qmd
└── exploratory/           # One-off analyses, quick tests
```

The numbers indicate the general flow of the analysis, not a strict linear dependency. Script 05 might read outputs from scripts 01 and 03 directly—the numbering just helps you understand the overall structure at a glance.

#### `scripts/exploratory/`

The `exploratory/` subdirectory is for one-off analyses, quick tests, and feasibility checks. These scripts:

- Don't need number prefixes or `BUILD_INFO.txt`
- Must never be depended on by other scripts (one-way dependency — exploratory scripts can read from `outs/`, but nothing reads from exploratory outputs)
- Can be cleaned out periodically without breaking anything
- Are good candidates for promotion to numbered scripts when an analysis matures

### `outs/` — Generated Outputs

**Every script gets a corresponding output folder.** Script `01_import_qc.qmd` writes to `outs/01_import_qc/`:

```
outs/
├── 01_import_qc/
│   ├── filtered_counts.rds
│   ├── qc_report.html
│   └── sample_summary.csv
├── 02_normalize/
│   └── normalized_counts.rds
├── 03_differential/
│   ├── limma_results.rds
│   └── significant_hits.csv
├── 04_volcano_plots/
│   ├── volcano_3min.pdf
│   ├── volcano_15min.pdf
│   └── volcano_30min.pdf
└── 05_heatmaps/
    └── cluster_heatmap.pdf
```

**This structure encodes provenance.** When you see a file in `outs/03_differential/`, you know exactly which script created it.

::: {.callout-warning title="Claude Code"}
Claude Code can review your project layout and suggest improvements based on the lab's conventions.

> Here's my project folder structure: [paste output of `ls -R`]. I'm analyzing RNA-seq data from three conditions with time points. Does this organization make sense?

Claude will compare your layout against the lab conventions, flag any issues (like outputs mixed with inputs), and suggest a cleaner structure.
:::

## How Dependencies Work

Scripts read from two places:

1. `data/` — external inputs
2. `outs/` — outputs from other scripts (typically earlier-numbered ones)

Dependencies don't have to be strictly linear. Here's an example dependency graph:

```{mermaid}
flowchart LR
    data["data/*"] --> s01["01_import_qc"]
    s01 --> s02["02_normalize"]
    s02 --> s03["03_differential"]

    s01 --> s05["05_heatmaps"]
    s03 --> s05
    data --> s05
```

In this example:

- Scripts 01 → 02 → 03 form a linear chain
- Script 05 reads from 01, 03, AND the original data
- The numbering reflects the logical order, not strict dependencies

## Documenting Dependencies

At the top of each script, explicitly list inputs and outputs:

```markdown
# Volcano Plots

**Inputs:**

- `data/sample_metadata.csv`
- `outs/03_differential/limma_results.rds`

**Outputs:**

- `outs/04_volcano_plots/volcano_*.pdf`
```

This makes dependencies grep-able:

```default
# Find which script produces a file
grep -r "limma_results.rds" scripts/

# Find which scripts depend on a file
grep -r "limma_results.rds" scripts/ | grep "Inputs:" -A5
```

## Setting Up Output Paths

In your setup chunk, define paths that match this structure:

```r
#| label: setup
#| include: false

library(tidyverse)
library(here)

# This script's output folder
dir_out <- here::here("outs", "03_differential")
dir.create(dir_out, recursive = TRUE, showWarnings = FALSE)

# Input paths
path_normalized <- here::here("outs", "02_normalize", "normalized_counts.rds")
path_metadata <- here::here("data", "sample_metadata.csv")
```

Then save outputs to `dir_out`:

```r
# Save results
saveRDS(results, file.path(dir_out, "limma_results.rds"))
write_csv(significant, file.path(dir_out, "significant_hits.csv"))

# Save figures
ggsave(file.path(dir_out, "volcano_plot.pdf"), p, width = 6, height = 4)
```

## Handling Subprojects

Projects with multiple distinct analyses (e.g., phosphoproteomics AND transcriptomics) need a way to group related scripts. There are two approaches:

### Prefixes (simpler)

Good for projects with 2–3 subprojects and fewer than ~5 scripts each:

```
scripts/
├── phospho_01_qc.qmd
├── phospho_02_normalize.qmd
├── phospho_03_differential.qmd
├── trans_01_import.qmd
├── trans_02_pca.qmd
└── combined_01_integration.qmd
```

Outputs mirror the naming:

```
outs/
├── phospho_01_qc/
├── phospho_02_normalize/
├── trans_01_import/
└── combined_01_integration/
```

### Subdirectories (recommended for larger projects)

When there are many scripts per section, subdirectories are cleaner. Numbering restarts at `01_` within each section:

```
scripts/
├── phosphoproteomics/
│   ├── 01_qc.qmd
│   ├── 02_normalize.qmd
│   └── 03_differential.qmd
├── transcriptomics/
│   ├── 01_import.qmd
│   └── 02_pca.qmd
├── combined/
│   └── 01_integration.qmd
└── exploratory/
```

Data and output folders mirror this structure:

```
data/
├── phosphoproteomics/
└── transcriptomics/

outs/
├── phosphoproteomics/
│   ├── 01_qc/
│   ├── 02_normalize/
│   └── 03_differential/
├── transcriptomics/
│   ├── 01_import/
│   └── 02_pca/
└── combined/
    └── 01_integration/
```

Either approach works—the key is that outputs always mirror the script organization so provenance is clear.

## Naming Conventions

### Scripts

- Use lowercase with underscores: `01_import_qc.qmd`
- Start with a two-digit number: `01_`, `02_`, ... `10_`, `11_`
- Use descriptive names: `03_differential.qmd` not `03_analysis.qmd`

### Output Files

- Name files descriptively: `normalized_counts.rds` not `data.rds`
- Use consistent extensions: `.rds` for R objects, `.csv` for tables, `.pdf` for figures
- For multiple similar outputs, use a pattern: `volcano_3min.pdf`, `volcano_15min.pdf`
- **Cross-language data**: When data produced by an R script will be read by a Python script (or vice versa), use **Parquet** format (`.parquet`). Avoid `.rds` (R-only) or `.pkl` (Python-only) for cross-language data. Both R (`arrow::write_parquet()`) and Python (`pd.to_parquet()`) handle Parquet natively.

### What to Avoid

- Spaces in filenames (use underscores or hyphens)
- Generic names (`output.csv`, `figure1.pdf`, `results.rds`)
- Date prefixes on every file (let git track versions)

## Version Control Considerations

### What to Commit

- All scripts (`scripts/*.qmd`)
- External data if small enough (`data/`)
- Environment files (`environment.yml`, `renv.lock`)
- README and documentation

### What to Ignore

Add to `.gitignore`:

```
# Large data files
data/large_file.h5ad

# Generated outputs (can be reproduced)
outs/

# Quarto rendering artifacts
*_files/
*_cache/
.quarto/
```

Whether to commit `outs/` is a judgment call:

- **Commit outputs** if they're small and you want them versioned
- **Ignore outputs** if they're large or easily regenerated

If you ignore outputs, document how to regenerate them in the README.

## Script Lifecycle

Every analysis script has a lifecycle. Track it with a `status` field in the YAML frontmatter:

```yaml
---
title: "Differential Expression"
status: development
---
```

| Status | Meaning | Location |
|--------|---------|----------|
| `development` | In active development; outputs are provisional | `scripts/` |
| `finalized` | Outputs are publication-ready; modify only with deliberate re-validation | `scripts/` |
| `deprecated` | Superseded; kept for reference | `scripts/old/` |

When deprecating a script, move it to `scripts/old/` and add `deprecated_by:` to the YAML:

```yaml
---
title: "Old Heatmaps"
status: deprecated
deprecated_by: 05_improved_heatmaps.qmd
---
```

This makes it clear which script replaced it, while git preserves the full history.

## Provenance: BUILD_INFO.txt

Every numbered script should write a `BUILD_INFO.txt` to its output folder as its last action. This answers "when was this output last regenerated, and from what code version?"

```
script: 01_analysis.qmd
commit: a1b2c3d
date: 2026-02-14 15:30:00
```

See the [Quarto](quarto.qmd) chapter for the R and Python code to generate this automatically.

## Helper Functions

### Project-level: `R/` and `python/`

Shared helper functions that multiple scripts in the project use live in dedicated directories:

```
project/
├── R/                    # Shared R helpers
│   └── helpers.R
├── python/               # Shared Python helpers
│   └── helpers.py
└── scripts/
```

Load them in your setup chunk:

```r
# R
source(here::here("R", "helpers.R"))
```

```python
# Python
import sys
sys.path.insert(0, str(PROJECT_ROOT / "python"))
from helpers import my_function
```

Don't version function names (`make_gene_short`, not `make_gene_short_v2`)—fix in place and let git track the history.

### Cross-project: `~/lib/`

When a helper function proves useful across 2+ projects, promote it to your personal library:

```
~/lib/
├── R/          # Cross-project R functions
└── python/     # Cross-project Python functions
```

This keeps project repositories clean while making reusable code accessible everywhere.

## Handling Old Versions

Analysis projects tend to accumulate old scripts and outputs. Here are the recommended approaches:

1. **Use git.** Delete old files; recover from history if needed. This is the preferred approach.

2. **Use `scripts/old/`.** For deprecated scripts you want to keep visible, move them to `scripts/old/` with `status: deprecated` in the YAML (see [Script Lifecycle](#script-lifecycle) above).

3. **Date sparingly.** Only add dates when you need multiple versions to coexist (e.g., comparing runs with different parameters):
   ```
   outs/03_differential/
   ├── 2025-01-15_strict_threshold/
   └── 2025-01-20_relaxed_threshold/
   ```

## Complete Example

Here's a well-organized phosphoproteomics project:

```
tryptamine_phospho/
├── .claude/
│   └── CLAUDE.md                    # Claude Code instructions
├── R/
│   └── gene_helpers.R               # Shared R functions
├── data/
│   ├── README.md                    # Documents data sources
│   ├── raw_counts.csv               # From mass spec core
│   ├── sample_metadata.csv          # Experimental design
│   └── gene_annotations.tsv         # Downloaded from UniProt
│
├── scripts/
│   ├── 01_import_qc.qmd            # status: finalized
│   ├── 02_normalize.qmd            # status: finalized
│   ├── 03_differential.qmd         # status: finalized
│   ├── 04_volcano_plots.qmd        # status: development
│   ├── 05_trajectory_clustering.qmd # status: development
│   ├── 06_heatmaps.qmd             # status: development
│   ├── exploratory/
│   │   └── test_new_clustering.qmd
│   └── old/
│       └── 04_volcano_plots_v1.qmd  # status: deprecated
│
├── outs/
│   ├── 01_import_qc/
│   │   ├── filtered_counts.rds
│   │   ├── qc_summary.html
│   │   └── BUILD_INFO.txt
│   ├── 02_normalize/
│   │   ├── normalized_counts.rds
│   │   └── BUILD_INFO.txt
│   ├── 03_differential/
│   │   ├── limma_results.rds
│   │   ├── significant_hits.csv
│   │   └── BUILD_INFO.txt
│   ├── 04_volcano_plots/
│   │   ├── volcano_3min.pdf
│   │   ├── volcano_15min.pdf
│   │   └── volcano_30min.pdf
│   ├── 05_trajectory_clustering/
│   │   ├── cluster_assignments.csv
│   │   └── elbow_plot.pdf
│   └── 06_heatmaps/
│       ├── heatmap_all_clusters.pdf
│       └── heatmap_per_cluster/
│
├── environment.yml
├── renv.lock
├── .gitignore
└── README.md
```

**Dependency flow:**

```{mermaid}
flowchart LR
    data["data/*"] --> s01["01"]
    s01 --> s02["02"]
    s02 --> s03["03"]
    s03 --> s04["04"]

    s02 --> s05["05"]
    s03 --> s05
    s05 --> s06["06"]
```

From this structure, anyone can understand:

- Where the raw data came from (`data/README.md`)
- The analysis workflow (numbered scripts)
- What each script produces (matching output folders)
- Which scripts are finalized vs. still in development (YAML `status:` field)
- When outputs were last generated (`BUILD_INFO.txt`)
- How to reproduce the analysis (environment files + scripts)
